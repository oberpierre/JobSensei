import logging
import json
from decouple import config
from llm import llm
from confluent_kafka import Consumer, KafkaError
from string import Template
from topic import Topic

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LlmConsumer:
  
    def __init__(self, kafka_conf):
        self.consumer = Consumer(kafka_conf)

    def __del__(self):
        if self.consumer:
            self.consumer.close()

    def _extract_json(self, raw_text):
        start = raw_text.find('{')
        end = raw_text.rfind('}') + 1
        if start == -1 or end == 0:
            logging.error("No JSON found in the response.")
            return None

        json_str = raw_text[start:end]
        try:
            json_obj = json.loads(json_str)
            return json_obj
        except json.JSONDecodeError as e:
            logging.error(f"Failed to parse JSON: {e}.")
        return None

    def _process_message(self, msg):
        topic = msg.topic()
        value = msg.value()
        record = json.loads(value.decode())

        logger.debug(f"Received message with topic {topic}: {record}")
        if topic == Topic.LLM_CATEGORIZE.value:
            template = Template(
"""Job listing:
$listing

From the provided job listing, extract and categorize details into a JSON format:
{
  "jobTitle": string,
  "role": {
    "team": string,
    "department": string,
  },
  "summary": string (autogenerated brief description based on the provided listing),
  "company": string,
  "location": {
    "city": string,
    "country": string (ISO 3166-1 alpha-2 country code, e.g. SG for Singapore)
  },
  "skills": string[],
  "employmentType": "full" | "part" | "temp" | "n/a" (default to n/a if not specified),
  "responsibilities": string[],
  "qualifications": {
    "required": string[],
    "preferred": string[]
  },
  "other": string[] (miscellaneous info not fitting into other properties)
}.
If properties are not specified and not inferrable in the listing, default to empty string or empty array corresponding to the properties data type.""")
            prompt = template.substitute(listing=record['content'])
            args = ['-ngl', '17', '-c', '2048']
            logger.info(f"Invoking LLM with prompt:\n{prompt}")
            response = llm(prompt, *args)
            res_json = self._extract_json(response)
            if res_json:
                logger.info(f"Response json: {res_json}")
            else:
                logger.error(f"Could not extract JSON from response: {response}")

    def start_processing(self):
        self.consumer.subscribe([Topic.LLM_CATEGORIZE.value])

        while True:
            msg = self.consumer.poll(1.0)

            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    logger.info(f'Reached end of partition {msg.partition()}')
                else:
                    logger.error(f'Error while consuming message: {msg.error()}')
            else:
                self._process_message(msg)

if __name__ == "__main__":
    kafka_conf = {
        'bootstrap.servers': config('BOOTSTRAP_SERVERS', default='kafka:9092'),
        'group.id': config('GROUP_ID', default='jobsensei-llm'),
        'max.poll.interval.ms': config('MAX_POLL_INVERVAL', default=1000000),
        'auto.offset.reset': 'earliest',    # Starts from beginning of topic, opposed to 'latest' which starts at the end
        'enable.auto.commit': True,         # Commit offset when message is successfully consumed
    }
    consumer = LlmConsumer(kafka_conf)
    consumer.start_processing()
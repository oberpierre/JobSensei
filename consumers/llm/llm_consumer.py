import logging
import json
from decouple import config
from llm import llm
from confluent_kafka import Consumer, KafkaError
from string import Template

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def _extract_json(raw_text):
    start = response.find('{')
    end = response.rfind('}') + 1
    if start == -1 or end == 0:
      logging.error("No JSON found in the response.")
      return None

    json_str = response[start:end]
    try:
      json_obj = json.loads(json_str)
      return json_obj
    except json.JSONDecodeError as e:
      logging.error(f"Failed to parse JSON: {e}.")
    return None

def process_message(msg):
    topic = msg.topic()
    value = msg.value()
    record = json.loads(value.decode())

    logger.debug(f"Received message with topic {topic}: {record}")
    if topic == 'jobsensei-llm-categorize-v1':
        template = Template(
"""Job listing:
$listing

From the provided job listing, extract and categorize details into a JSON format:
{
  "jobTitle": string,
  "role": {
    "team": string,
    "department": string,
  },
  "summary": string (autogenerated brief description based on the provided listing),
  "company": string,
  "location": {
    "city": string,
    "country": string (ISO 3166-1 alpha-2 country code, e.g. SG for Singapore)
  },
  "skills": string[],
  "employmentType": "full" | "part" | "temp" | "n/a" (default to n/a if not specified),
  "responsibilities": string[],
  "qualifications": {
    "required": string[],
    "preferred": string[]
  },
  "other": string[] (miscellaneous info not fitting into other properties)
}.
If properties are not specified and not inferrable in the listing, default to empty string or empty array corresponding to the properties data type.""")
        prompt = template.substitute(listing=record['content'])
        args = ['-ngl', '17', '-c', '2048']
        logger.info(f"Invoking LLM with prompt:\n{prompt}")
        response = llm(prompt, *args)
        json = _extract_json(response)
        if json:
          logger.info(f"Response json: {json}")
        else:
          logger.error(f"Could not extract JSON from response: {response}")

def consume_messages():
    conf = {
        'bootstrap.servers': config('BOOTSTRAP_SERVERS', default='kafka:9092'),
        'group.id': config('GROUP_ID', default='jobsensei-llm'),
        'max.poll.interval.ms': config('MAX_POLL_INVERVAL', default=1000000),
        'auto.offset.reset': 'earliest',    # Starts from beginning of topic, opposed to 'latest' which starts at the end
        'enable.auto.commit': True,         # Commit offset when message is successfully consumed
    }

    consumer = Consumer(conf)
    consumer.subscribe(["jobsensei-llm-categorize-v1"])

    while True:
        msg = consumer.poll(1.0)

        if msg is None:
            continue
        if msg.error():
            if msg.error().code() == KafkaError._PARTITION_EOF:
                logger.info(f'Reached end of partition {msg.partition()}')
            else:
                logger.error(f'Error while consuming message: {msg.error()}')
        else:
            process_message(msg)

    consumer.close()

if __name__ == "__main__":
    consume_messages()